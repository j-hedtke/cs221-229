{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from bert_embedding import BertEmbedding\n",
    "from scipy import spatial\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "\n",
    "## Min/max distance between two points in dataset\n",
    "dmin = 0.0604\n",
    "dmax = 0.8\n",
    "\n",
    "## Approximation factor\n",
    "C = 1.005\n",
    "\n",
    "## Size of hash function output\n",
    "K = 10\n",
    "\n",
    "## Number of hash functions drawn\n",
    "L = 20\n",
    "\n",
    "## 1 - delta = Success probability of PLEB\n",
    "delta = 0.001\n",
    "num_hash_table_constructions = math.ceil(math.log(1.0/(1 - delta), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING - get sentences from text file\n",
    "\n",
    "dirname = os.path.dirname(os.path.abspath(''))\n",
    "with open(os.path.join(dirname, 'data/datacleaned_valid.txt'), encoding=\"utf8\") as fp:\n",
    "    phrases = fp.read().split('\\n')\n",
    "phrases_list = [list(filter(None, line.strip().split(','))) for line in phrases if line.strip() and re.search('[a-zA-Z]', line)]\n",
    "sentences = [sentence for phrase in phrases_list for sentence in phrase]\n",
    "\n",
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_sentence(sentence):\n",
    "    average = np.zeros(768,)\n",
    "    sentence_embedding = bert_embedding([sentence])[0]\n",
    "    for word_embedding in sentence_embedding[1:]:\n",
    "        for token in word_embedding:\n",
    "            average += token\n",
    "    return np.divide(average, len(sentence_embedding)-1)\n",
    "\n",
    "def get_bert_sentences(sentences):\n",
    "    bert_sentences = []\n",
    "    sentence_to_bert_sentence = defaultdict(list)\n",
    "    bert_sentence_to_sentence = defaultdict(list)\n",
    "    for sentence in sentences:\n",
    "        bert_sentence = get_bert_sentence(sentence)\n",
    "        bert_sentences.append(bert_sentence)\n",
    "        sentence_to_bert_sentence[sentence] = bert_sentence\n",
    "        bert_sentence_to_sentence[str(bert_sentence)] = sentence\n",
    "    return (bert_sentences, sentence_to_bert_sentence, bert_sentence_to_sentence)\n",
    "\n",
    "def sentence_pair_dist(sentence1, sentence2):\n",
    "    bert_sentence1 = get_bert_sentence(sentence1)\n",
    "    bert_sentence2 = get_bert_sentence(sentence2)\n",
    "    return spatial.distance.cosine(bert_sentence1, bert_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING - get bert embeddings of sentences\n",
    "bert_sentences, sentence_to_bert_sentence, bert_sentence_to_sentence = get_bert_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dot_product(dot_product):\n",
    "    if dot_product > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def random_hyperplane_round(bert_sentence, hyperplane):\n",
    "    dot_product = np.dot(bert_sentence, hyperplane)\n",
    "    return round_dot_product(dot_product)\n",
    "\n",
    "\n",
    "def get_random_hyperplane_hash_function(bert_sentence_size, size=K):\n",
    "    return [np.random.normal(size=bert_sentence_size) for _ in range(size)]\n",
    "\n",
    "\n",
    "def eval_hyperplane_hash_function(hash_function, bert_sentence):\n",
    "    return [random_hyperplane_round(bert_sentence, hyperplane) for hyperplane in hash_function]\n",
    "\n",
    "\n",
    "def construct_hash_table(bert_sentences):\n",
    "    hash_function = get_random_hyperplane_hash_function(bert_sentence_size=len(bert_sentences[0]))\n",
    "    hash_table = defaultdict(list)\n",
    "    for bert_sentence in bert_sentences:\n",
    "        key = eval_hyperplane_hash_function(hash_function, bert_sentence)\n",
    "        hash_table[str(key)].append(bert_sentence)\n",
    "    return (hash_function, hash_table)\n",
    "\n",
    "\n",
    "def construct_hash_tables(bert_sentences, num_tables=L):\n",
    "    hash_tables = []\n",
    "    for _ in range(L):\n",
    "        hash_tables.append(construct_hash_table(bert_sentences))\n",
    "    return hash_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pleb(dist, hash_table_constructions, r2, query_bert_sentence, query_cutoff=4*L):\n",
    "    for hash_tables in hash_table_constructions:\n",
    "        num_queries = 0\n",
    "        for hash_function, hash_table in hash_tables:\n",
    "            key = eval_hyperplane_hash_function(hash_function, query_bert_sentence)\n",
    "            for candidate_bert_nn in hash_table[str(key)]:\n",
    "                if dist(candidate_bert_nn, query_bert_sentence) < r2:\n",
    "                    return candidate_bert_nn\n",
    "                num_queries += 1\n",
    "                if num_queries >= query_cutoff:\n",
    "                    break\n",
    "            if num_queries >= query_cutoff:\n",
    "                    break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pleb_instances = math.ceil(math.log((dmax/(C-1.0))/(dmin/(2.0*C)), C))\n",
    "pleb_instances = []\n",
    "for index in range(num_pleb_instances):\n",
    "    pleb_instance = {}\n",
    "    pleb_instance['r2'] = dmin/(2.0)*(C**index)\n",
    "    pleb_instance['hash_table_construction'] = [construct_hash_tables(bert_sentences) for _ in range(num_hash_table_constructions)]\n",
    "    pleb_instances.append(pleb_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ann(query_sentence, pleb_instances):\n",
    "    query_bert_sentence = get_bert_sentence(query_sentence)\n",
    "    lo = 0\n",
    "    mid = math.floor(num_query_distances/2)\n",
    "    high = num_query_distances - 1\n",
    "    bert_ann = None\n",
    "    while lo <= high:\n",
    "        pleb_instance = pleb_instances[mid]\n",
    "        candidate_bert_nn = solve_pleb(spatial.distance.cosine, pleb_instance['hash_table_construction'], pleb_instance['r2'], query_bert_sentence)\n",
    "        if candidate_bert_nn is None:\n",
    "            lo = mid + 1\n",
    "            mid = math.floor((lo + high)/2)\n",
    "        else:\n",
    "            bert_ann = candidate_bert_nn\n",
    "            high = mid - 1\n",
    "            mid = math.floor((lo + high)/2)\n",
    "    ann = bert_sentence_to_sentence[str(bert_ann)]\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i canâ€™t hear you let me turn the volume up\n",
      "0.23102311702435696\n"
     ]
    }
   ],
   "source": [
    "## query_sent is the sentence you're finding the approximate nearest neighbor for (ann). To solve,\n",
    "## use solve_ann, which takes query_sent and pleb_instances (this is part of preprocessing, so after you\n",
    "## run it above, you don't need to touch it). It returns an approximate nearest neighbor from\n",
    "## the dataset.\n",
    "\n",
    "query_sent = 'i can\\'t hear you'\n",
    "ann = solve_ann(query_sent, pleb_instances)\n",
    "print(ann)\n",
    "print(sentence_pair_dist(query_sent, ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
