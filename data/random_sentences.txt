In this section we demonstrate the workflow by training and
testing a model for salt body interpretation. The seismic image
in this example is cropped from SEAM Phase 1 synthetic
dataset. We selected 8 crossline 2D
slices as training data. The training labels are manual annotations
generated by an optimal path picking method (Wu et al.,
2017). With a seismic amplitude image as input, the network
should identify the entire salt body as a probability map output
starting from a seed point.
We set the FoV size for 2D salt body interpretation to 127
127. Although it is a relatively small size compared to
400500, the size of the target, the movement of the FoV
should be able to cover the entirety of the salt body. In the network,
we downsample the inputs 3 times with 2 max pooling
every 2 convolutional layers (6 layers) in the encoder section
and symmetrically upsample in the decoder section. Since the
dimension of the inputs is downsampled 8 in total before upsampling,
the FoV is zero-padded to 128128, and cropped
to the original size at the final output.
In order to train the model to move the FoV effectively within
the geobody, we adopt a “dynamic subregion” scheme to constrain
the movements of the FoV during training. A limited
number of subregions are randomly sampled from each training
image so that all subregions are centered within the valid
geobody. In the first epoch, subregions share the same size
with the FoV, so the network only learns how to segment the
input image without actually moving the FoV. After that, the
subregions grow in size until hitting the 256256 limit on
each new epoch. This way, the network gradually learns to
move the FoV based on previous segmentation results further
and further away from the starting point. We train the model
for 5 epochs and use area under the curve (AUC) (Hanley and
McNeil, 1982) as the metric to determine the matching performance
of the prediction likelihood against the ground truth.
The training took less than 5 hours with a GTX 1080-Ti GPU. Note that because the FoV cannot move outside the image
boundary, FoV centers could not cover the entire range of the
picked fault; however, the segmentation of that fault is already
completed and flood-filling can continue as shown in Figure 6.
Figure 6(a, d) show examples of picking only one fault instance
with other faults in the vicinity. Figure 6(a) shows tracking
history when the seed point is put on the middle fault, and
in Figure 6(d) the flood-filling can continue to fill up the entire
instance. Figure 6(b, c, e, f) show similar processes but start
with different seed points to the left and right side of the one in
Figure 6(a, d). In Figure 6(b, e), even though the seed point is
slightly off from the correct fault position, flood-filling can automatically
correct to align with the fault. Note that although
the tracking is focused on one of the many faults, the output
fault likelihood maps overlaid on the background image show
that the model is still aware of the other faults in the vicinity,
depending on the size of the FoV. Figure 7 shows all three fault
instances generated in Figure 6. This demonstrates the ability
to separate geobody instances with the same attribute.
Although all examples are in 2D, the transfer of this concept
to 3D is straightforward and requires trivial effort. In 3D, the
proposed workflow can be more computationally efficient than
the traditional sliding window scheme. In addition to the salt body and fault interpretation, the proposed method can be additionally
suitable for 3D channel tracking.
On the other hand, the proposed method may not perform well
in the case of trying to separate instances that overlap. This is
due to the fact that each FoV search for the proposal for the
next movement in all directions equally in order to fill the geobody
space. However, show that deep learning
can predict not only geobody likelihood but also relevant attributes,
 fault dip and strike angle. It is worth investigating
to utilize these important directional attributes in deep learning
interpretation methods, such as guiding the flood-filling search
pattern so that overlapping instances can be separated. Figure 6 shows the predicted results after implementing
prediction on the whole dataset by using trained model.
From Figure 6, high accuracy can be seen on the sections
used for training and sections in the neighborhood. Both
positions and boundaries of seismic facies are generally
predicted correctly. It is obvious that prediction on training
sections such as inline 320 achieves high similarity to label
images, although some tiny details need better prediction.
Several seismic facies are predicted particularly precisely in
Figure 6(b), for example, high amplitude continuous (in
green), grizzly (in yellow), high amplitude (in red) and salt
dome (in brown). It takes 5 hours 59 minutes in training and
only 1.3 seconds in one-section prediction on two NVIDIA
Tesla K40m GPUs.
Compared with CNNs applied for seismic facies
interpretation (Dramsch et al., 2018; Zhao, 2018), our
scheme significantly reduces computational time in training
and predicting. Meanwhile, seismic facies are classified
more accurately by DeepLabv3+. Compared with simple
encoder-decoder models in seismic facies interpretation
(Zhao, 2018; Di, 2018), our scheme predicts more accurate
results for encoding multi-scale information and decoding
more details.
Good performance can be observed even in crossline
sections, time-slice sections, and the inline sections far from
training sections, as shown in Figure 7. These predicted
sections are randomly extracted from the predicted data cube.
And note that the size of each training slice is 513􀵈513 while
data sections to be predicted are at size 398􀵈886. It means
that DeepLabv3+ is flexible in size during training and
predicting.
We propose an effective scheme for automatic seismic facies
interpretation by utilizing DeepLabv3+. This enhanced
encoder-decoder structure produces more accurate pixellevel
predictions by extracting multi-scale features and
recovering more details. And this scheme reduces weights to
significantly improve efficiency. The implementation on F3
dataset shows its good performance in seismic facies
interpretation. Furthermore, the well predicted results
indicate huge potentials in seismic interpretation using
DeepLabv3+. Seismic facies commonly indicate types of lithological
association and sedimentary characteristics of geological
bodies (Brown, 2011). The interpretation of seismic facies
provides a reference for analyzing subsurface geological
conditions. Conventional seismic interpretation methods
usually require a lot of manual works, and the interpretation
results are inevitably affected by some subjective factors. To
address these issues, deep learning has drawn a lot of
attention in recent years.
Research in deep learning has made significant progress in
several fields such as computer vision. Considering the
similarity between seismic interpretation and computer
vision, many researchers adopt computer vision algorithms
to analyze and understand subsurface structures (AlRegib et
al., 2018). Convolutional neural networks (CNN) (LeCun et
al., 1989, 1998) are commonly considered to solve seismic
interpretation problems, such as faults identification (Wu et
al., 2018), salt dome prediction (Waldeland and Solberg,
2017; Waldeland et al., 2018) and seismic facies
interpretation (Dramsch et al., 2018). However, CNN has a
fatal shortage that CNN merely predicts image-wise
classification results instead of pixel-wise results. Although
dense prediction can be achieved by performing
classification using CNN models on small patches of data to
infer the class label at the patch center (Zhao, 2018), it is
inaccurate and computationally time-consuming.
The FCN model (Shelhamer et al., 2017) developed from
CNN inspires researchers to establish encoder-decoder
structures for semantic segmentation tasks. These structures
are specially designed for dense predictions because they can
predict pixel-level outputs in real time. The encoder similar
to a CNN model is used to extract features to obtain feature
maps, and the decoder similar to a deconvolution operation
is used to recover feature maps to the original size and
predict outputs. Simple encoder-decoder models have been
implemented in salt dome detection (Shi et al., 2018) and
facies interpretation (Di et al., 2018; Zhao, 2018).
DeepLabv3+ (Chen et al., 2018) is an enhanced encoderdecoder
structure that achieves a state-of-the-art
performance on PASCAL VOC 2012 and Cityscapes
datasets. This structure performs higher accuracy than
simple encoder-decoder models by applying Atrous spatial
pyramid pooling (ASPP) module and a simple yet effective
decoder module. By utilizing depthwise separable
convolution, DeepLabv3+ can reduce a lot of weights and
greatly improve the efficiency. In this paper, we propose a
scheme for seismic facies interpretation using DeepLabv3+.
After implementing effective data augmentation to generate
diversity training samples, we perform DeepLabv3+ to
extract multi-scale features of seismic facies and recover
more details in prediction results. By tuning parameters, the
application of our scheme on F3 dataset demonstrates good
performance for high accuracy and efficiency.After data augmentation, we obtain thousands of diversity
data slices at crop size 321􀵈321 or 513􀵈513. All data slices
are set as training data, and the 30 training sections without
being cropped are set as evaluation data. Test data and data
for predicting are also complete seismic sections without
being cropped in slices. We test several sets of parameters
including base learning rate, data crop size, atrous rates in
ASPP module, output stride, and number of data slices for
training after data augmentation. These parameters may
affect prediction for training the encoder-decoder structure.
The performance is measured by mean intersection over
union (mIoU) (Alberto et al., 2017) across the 9 facies.
Higher value of mIoU represents better performance. The
details and quantitative comparison results are listed in Table
1.
By comprehensive comparison of prediction results under
different parameters, a set of parameters for performing best
both in accuracy and efficiency are selected as the final
scheme. We train 1500 data slices on DeepLabv3+ with base
learning rate = 0.002, data crop size = 513×513 and atrous
rates = [12, 24, 36] with output stride = 8. In (Alaudah et al., 2018a), we proposed a method for automatically
obtaining large amounts of weakly-labeled data
using two main steps; similarity-based retrieval and weaklysupervised
label mapping. The only requirement is for the interpreter
to “define” the various classes by selecting at least
one exemplar image for every class. Due to the limited space,
we provide a brief explanation only, and we refer to our previous
work for the details.
Similarity-based retrieval takes exemplar images for each seismic
facies, like the ones shown in Figure 1, and then retrieves
thousands of images with similar facies to the exemplar images.
In our work, two exemplar images for every class were
sufficient. Figure 1 shows the exemplar images for different facies
(indicated by different colors) from our recently released
dataset (Alaudah et al., 2019) and shows a small subset of the
retrieved images for every class. In this work, we retrieve 1000
images for every class of seismic facies.
Given the retrieved images, the weakly-supervised label mapping
algorithm learns the common features in each class, and
then uses these common features to predict the pixel-level labels
for all the retrieved images. Figure 2 shows the label
mapping results for various images. The label mapping algorithm
also produces confidence values, denoted q(x), in each
label it predicts. In the next subsection, we explain how these
confidence values are used in the network loss function of our
weakly-supervised models.To train and test our different models, we use our recently released
facies classification dataset (Alaudah et al., 2019). This
dataset includes six different lithostratigraphic classes from the
Netherlands F3 block. There are (from shallowest to deepest):
the Upper, Middle, and Lower North Sea groups, in addition
to the Rijnland/Chalk, Scruff, and Zechstein groups. Figure 3
shows a 3D view of the survey with the Scruff and Zechstein
groups visible. We use 75⇥75 patches extracted from the NW
region of the survey for training, and we test our models on
test set #1 and #2 as shown in Figure 3.
For a fair comparison between strongly- and weakly-supervised
models, we use the same underlying architecture for both. We
use an encoder-decoder style architecture similar to SegNet
(Badrinarayanan et al., 2017). This architecture performs well
for semantic segmentation tasks and outputs a prediction that
matches the size of the network input. In our case, the output
predictions are six 75⇥75 patches, corresponding the six different
classes in our dataset. In addition to the baseline case,
we also test the same models with data augmentation3 and skip
connections. We use 10% of our training data for validation,
and we train all our models until convergence. We then apply
these models on the two test sets in a sliding window fashion
and average the resulting heat maps for the six classes. Finally,
we compute our evaluation metrics on both test sets, and report
the results in Table 1.
We use several metrics to evaluate our models. Namely, pixel
accuracy (PA), class accuracy (CA) for each individual class,
mean class accuracy (MCA) which averages the CA score for
all classes, and frequency-weighted intersection over union (FWIU).
These metrics are detailed in (Alaudah et al., 2019).
Figure 4 shows sample results from inline #200 in test set #1
for both our strongly- and weakly-supervised models. Also,
Table 1 summarizes the objective results that we have obtained.
In the strongly-supervised model, adding skip connections and
augmentation to the baseline both helped improve the results
over the baseline model. We note that in Figure 4 the stronglysupervised
results are rather good, except for the two models
3This involves randomly rotating every training patch by up to±25#, adding random Gaussian
noise, and randomly flipping the training patches horizontally.
confusing the features in the Scruff class with the ones in the
Upper North Sea class. Section-based models that train on the
entire section, not only patches, can help remedy this issue.
On the other hand, our preliminary weakly-supervised results
show that using data augmentation and increasing the value of
g to let the network focus on harder misclassified regions can
help improve the results. Furthermore, when we weigh the loss
function of each image by its similarity to the exemplar image
used to retrieve it (by setting b = 1), we can also improve the
PA and FWIU scores by about 2%. There are several ways
where these weakly-supervised results can be improved, this
includes the use of more data augmentation during training and
testing, adding regularization to the network to prevent it from
overfitting to the weak labels, and better exploitation of side
information such as confidence and similarity values. Finally,
we note that our weakly-supervised results are still preliminary
ones, and better weakly-supervised results will be shared in the
presentation. We use the recently released facies classification benchmark
to train and test various strongly- and weakly-supervised models
for facies classification. We compare the results and these
models to study the practicality of using weakly-supervised
models for facies classification. The weakly-supervised models
only require two images annotated on the image-level, compared
to our strongly-supervised models that used 400 fullyannotated
seismic inlines for training. Naturally, the stronglysupervised
models perform better than weakly-supervised ones.
While our weakly-supervised results are encouraging given the
minimal amount of annotated data they require, we still believe
more research is needed to help minimize the performance gap
between weakly- and strongly-supervised models. For both models, we experiment with using data augmentation
to artificially increase the size of the training set.
Data augmentation applies different label-preserving transformations
to the training data such as rotation, random horizontal
flipping, and the addition of Gaussian noise.
